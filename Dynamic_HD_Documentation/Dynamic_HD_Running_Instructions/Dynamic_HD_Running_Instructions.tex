\documentclass{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{a4paper}                   % ... or a4paper or a5paper or ...
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{epstopdf}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{url}
\lstdefinestyle{bash_input}{
language=bash,
basicstyle=\small\sffamily,
numbers=left,
numberstyle=\tiny,
numbersep=3pt,
frame=tb,
columns=fullflexible,
backgroundcolor=\color{yellow!20},
linewidth=0.9\linewidth,
xleftmargin=0.1\linewidth
}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Instruction for Running the Dynamic HD Parameter and Restart File Generation Code Version 3.7}
\author{Thomas Riddick}
\date{24th June 2020}
\begin{document}
\lstset{language=bash}
\maketitle
\section{Overview}
This document provides instructions for using the dynamic hydrological discharge river routing and flow parameter generation scripts. The first set of instructions, presented in section \ref{section-max-planck-instructions}, concentrates on their use on the Max Planck Institute System; however, this should also be read through by external users as it provides a general overview the main input requirements of the script. Given the wider range of the linux systems used in the geosciences I can't provide a detailed set of instructions for systems outside our institute but I give some general guidance in section \ref{section-external-instructions} including suggests how this code might be used without using the Anaconda package and environment manager.

The HD model in JSBACH requires two files to run. One is the hdpara.nc file. This contains the flow directions and the water retention co-efficients (a measure of how quickly water flows) of each grid cell on the HD models regular latitude-longitude $0.5$ degree grid. The other is the hdstart.nc. This contain the initial water content of each grid cell (split into various reservoirs). A user generated hdstart.nc file is only required at the start of run; later restart of the model should use the hdstart.nc restart file provided in the set of restart files of the previous section of the run.

The Dynamic HD code always generates a hdpara.nc file. It only generates a hdstart.nc file at the very start of a run (i.e. when this dynamic HD code is first called).

The code will normally run in about $20$~seconds on a quad-core  3.5GHz i7 Linux machine with 16GB of RAM (assuming the machine is otherwise unloaded) or about $40$~second on the `Mistral' super computer (individual processors of this tending have a somewhat lower clock speed than 3.5GHz). However the first use of this code in a run will take longer (maybe $5$-$10$ minutes) because the Cython, C++ and Fortran code within the script must be cleaned, compiled and built. Subsequent runs don't recompile and hence are fast.

The code is provided in two repositories. The vast bulk of the code is provided in the main repository which is available under a 3-clause BSD open source license from \url{https://github.com/ThomasRiddick/DynamicHD} or \url{https://doi.org/10.5281/zenodo.1208066}. A small subsection of the code required for the generation of flow parameters (but not river directions) is contained in a git submodule should be placed in the folder \\\lstinline[style=bash_input]{Dynamic_HD_bash_scripts/parameter_generation_scripts} within the main repository; this is only available under the Max Planck Institute for Meteorology Software License Version 2 by application to the author (it can also be found in the \lstinline[style=bash_input]{contributions} directory of JSBACH). Given that the flow parameters would likely not apply to any model other the JSBACH HD model it is unlikely external users would actually need this code unless they had access to JSBACH (which would imply they had already signed up to the MPI software license). If this submodule is not included then the script will still run and produce a river directions only hdpara.nc file.

The flow directions are given in both an index based and 1-9 numeric keypad format in the hdpara.nc file. This is only available upon signing the  It would be quite possible to use these in a hydrological discharge model other than the HD model of JSBACH; obviously after converting them to the correct format for that other model. If this alternative hydrological discharge model required flow parameters the
dynamic HD model scripts could be adapted to produce these alternative flow parameters directly. Unfortunately, although some initial work has been done to separate out the flow parameter generation code contained in the submodule from the main river routing generation code, there is currently no way to turn off calls to the flow parameter generation code automatically. This must be done by editing the scripts by hand; I will outline how this might be done below. Internally we use the Max Planck Institute for Meteorology's (and DKRZ's) module system to manage help manage the external applications. The generation of new start files is likely specific to the HD model's treatment of flow parameters; thus unless coupled with the HD model it is likely this will be of little use and should also be edited out by hand. This obviously won't be available to external users who will need to find a replacement for it. I also use the conda package manager and the associated Anaconda repository (which focusses on managing python and R packages). This is freely available online (see \url{https://www.anaconda.com}; conda itself is open source as is the Anaconda distribution; they are managed by a firm which is also called Anaconda).

The instructions provided here have not been subject to rigorous testing as the code evolves continually and I don't have the resources to test this instruction set with each new release. However, they should provide a framework to guide any attempt to setup and run the dynamic hydrology scripts. In the case of problems it is recommend to contact the author.
\section{Instructions for use on the Max Planck Institute for Meteorology systems} \label{section-max-planck-instructions}
All command issued on the command line are for the bash shell. Any lines without line numbers are merely continuations of the previous line and not separate commands.
\begin{enumerate}
\item Create an area to run the code in and populate it with the necessary subdirectories. For example:
\begin{lstlisting}[style=bash_input]
mkdir dynamic_hd_code_area
cd dynamic_hd_code_area
mkdir workdir
mkdir outputdir_step1
mkdir outputdir_step2
\end{lstlisting}
\item Download the dynamic HD code and switch the repository to version 3.7. (Be sure to perform this last step; older versions probably won't work anymore and if they do then they will likely give erroneous results.). The code is stored on Github; you could also download it from Zenodo.
\begin{lstlisting}[style=bash_input,breaklines=true]
git clone https://github.com/ThomasRiddick/DynamicHD.git Dynamic_HD_Code
cd Dynamic_HD_Code
git checkout release_version_3.7
cd ..
\end{lstlisting}
\item Download the parameter generation scripts (which are a separate submodule that is nested inside the dynamic HD code). The code is stored in my home area. Thus you can change  \lstinline[style=bash_input]{dattel} (the name of my machine) to the name of your own machine if you wish.
\begin{lstlisting}[style=bash_input,breaklines=true]
cd Dynamic_HD_Code/Dynamic_HD_bash_scripts
git clone git+ssh://dattel.mpimet.mpg.de/home/mpim/m300468/workspace/Dynamic_HD_Code/Dynamic_HD_bash_scripts/parameter_generation_scripts
cd ../..
\end{lstlisting}
\item Retrieve the necessary ancillary data from my home area. (Replacing \lstinline[style=bash_input]{m300468@dattel} with your own machine and user name.)
\begin{lstlisting}[style=bash_input,breaklines=true]
scp -r m300468@dattel.mpimet.mpg.de:/home/mpim/m300468/HDancillarydata .
\end{lstlisting}
Note the ``.'' as the second argument here!
\item Edit the paths in the top level configuration file. (Of course you can use any editor you like; you don't have to use gvim.)
\begin{lstlisting}[style=bash_input]
cd HDancillarydata
gvim top_level_driver.cfg
\end{lstlisting}
\lstinline[style=bash_input]{top_level_driver.cfg} is sourced by the top level run driver script when the dynamic HD code runs. This means that like the top level run script itself it is written in bash (but doesn't start with \lstinline[style=bash_input]{#!/bin/bash} as it is sourced rather than run). This config file must define the variables \lstinline[style=bash_input]{source_directory}, \lstinline[style=bash_input]{no_conda} and \lstinline[style=bash_input]{no_modules} and should do nothing else besides. \lstinline[style=bash_input]{source_directory} should be the path to the \lstinline[style=bash_input]{Dynamic_HD_Code} directory downloaded from git. (For users of older version note the variable \lstinline[style=bash_input]{external_source_directory} is no longer required.) \lstinline[style=bash_input]{no_conda} and \lstinline[style=bash_input]{no_modules} should both be set to  \lstinline[style=bash_input]{"false"}. The format required should be clear from the original version. For this variable please use the full absolute path. An example of the content of this file once edited might be:
\begin{lstlisting}[style=bash_input,breaklines=true]
source_directory="$HOME/dynamic_hd_code_area/Dynamic_HD_Code"
no_conda="false"
no_modules="false"
\end{lstlisting}
Once you have finished editing close the editor and then switch back to the main directory of your dynamic HD setup:
\begin{lstlisting}[style=bash_input]
cd ..
\end{lstlisting}
\item Retrieve the orography, glacier mask and land-sea mask you wish to use along with the present day orography your orography is based upon. For example you can retrieve the example data from my home area (replacing \lstinline[style=bash_input]{m300468@dattel} with your username and machine name):
\begin{lstlisting}[style=bash_input,breaklines=true]
scp -r m300468@dattel.mpimet.mpg.de:/home/mpim/m300468/HDexampledata/* .
\end{lstlisting}
The data should be on a normal (\textbf{non-gaussian}) $10$-minute latitude-longitude grid. Grid boxes coordinate and values should be for their central point. The first column of cells on the $x$-axis should be cells centred on the Greenwich Meridian (i.e. exactly $0$\degree~longitude). The first row of cells on the $y$-axis should touch the south pole with their southern edge and last row of cells on the $x$-axis should touch the north pole on their northern edge. This should produce a right-way up (when North at the top is seen as the right way up) image in ncview. Heights should be in metres; the datum is not important so long as it is consistent. The orography should preferable extend below the ocean although this is not strictly necessary (however if it does not then potential errors could occur if the supplied land-sea mask doesn't match exactly the extent of the provided orography data). The land-sea mask should be binary with values of $0$ (for land points) and $1$ for sea points. The glacier mask should supply the percentage of grid cells covered by ice from $0$\% to $100$\%;  this will then be converted to a binary mask of either glacier  present or no glacier present.  The format of the files must be netCDF (extension \lstinline[style=bash_input]{.nc}) and the orography fields and land-sea mask field should be named one of \texttt{Topo}, \texttt{topo}, \texttt{field\_value}, \texttt{orog}, \texttt{z}, \texttt{ICEM}, \texttt{DEPTO}, \texttt{usurf}, and \texttt{bats}. If more than one field in the file has one of these names then the names are tried in the order above until the first match is found. (Thus the land-sea mask and orography must come in separate files.) These field names can be changed (thus allowing the landsea mask and orography to come in the same file) by altering a configurations file; more details are given in section \ref{section-notes}. The glacier mask field should be called \texttt{sftgif}. The present day orography supplied \textbf{must} be the exact present day orography that was used as basis to create the orography for the past time slice being processed. \textbf{This is important and if this criterion is not met then extremely unreliable results will be obtained.} The example data supplied here meets all these requirements and it may be therefore a good idea to compare any new data against it to make sure the new data also meets these requirements.
\item Run the dynamic HD code for the first time-step to obtain both a hdpara.nc and hdstart.nc file. The top level script is located in the directory \texttt{Dynamic\_HD\_Code} at \path{Dynamic_HD_Code/Dynamic_HD_bash_scripts/dynamic_hd_top_level_driver.sh} and takes twelve command lines arguments:
\begin{enumerate}[i)]
\item A flag (set to either \texttt{T} or \texttt{F}) to indicate if this is the first time-step. So in this case set this to \texttt{T}.
\item The file path of the input orography file. This can be relative or absolute file path.
\item The file path of the input land-sea mask file. This can be relative or absolute file path.
\item The file path of the present day base orography the input orography was derived from. This can be a relative or absolute file path.
\item The file path of the file containing the input glacial mask. This can be a relative or absolute file path.
\item The target file path for the the output hdpara.nc file. This can be relative or absolute file path.
\item The directory path of the ancillary data directory. This can be relative or absolute file path.
\item The directory path of the working directory. This can be a relative or absolute path. It is strongly advised to use a empty directory for this as any loose .nc files in this directory will be moved to the diagnostic output directory at the end of the script and labelled with the time in the run and run ID. If this directory doesn't exist it will be created.
\item The directory path of the diagnostic output directory (which must exist). This can be relative or absolute file path.
\item The diagnostic data experiment ID label. This should be a string and will be appended to any diagnostic data output files.
\item The diagnostic data time within run. This should be a string and will be appended to any diagnostic data output files after the experiment ID.
\item The target file path for the the output hdstart.nc file. This can be relative or absolute file path. This is only produced when the first argument is  \texttt{T}.
\end{enumerate}
For example:
\begin{lstlisting}[style=bash_input,breaklines=true]
./Dynamic_HD_Code/Dynamic_HD_bash_scripts/dynamic_hd_top_level_driver.sh T ./Ice6g_c_VM5a_10min_21k.nc ./10min_ice6g_lsmask_with_disconnected_point_removed_21k.nc ./Ice6g_c_VM5a_10min_0k.nc ./Ice6g_c_VM5a_10min_21k.nc hdpara.nc ~/HDancillarydata ./workdir/ ./outputdir_step1/ aa01 21000 hdstart.nc
\end{lstlisting}
As noted previously, the first run will compile the necessary FORTRAN, C++ and Cython code as it goes along and will produce an hdstart.nc file. This run will thus take longer than a normal run; perhaps $5$~minutes. Output (other than the hdpara.nc and hdstart.nc files) will be moved from the working directory to the given diagnostic output directory by the final section of the script. \textbf{It is important to check the order of the command line arguments is correct as it might be possible for the script to run successfully yet give highly erroneous results if the settings are not in the correct order.} 
\item Run the dynamic HD code for subsequent time-steps to obtain a new hdpara.nc. Use this with the hdstart.nc file given as a restart file by the previous section of the model run. The top level script and command lines arguments are same as for the first step run but the twelfth and final command line argument is now not required or permitted. And of course the flag should be set to \texttt{F} for the first command line argument. For example:
\begin{lstlisting}[style=bash_input,breaklines=true]
./Dynamic_HD_Code/Dynamic_HD_bash_scripts/dynamic_hd_top_level_driver.sh F ./Ice6g_c_VM5a_10min_21k.nc ./10min_ice6g_lsmask_with_disconnected_point_removed_21k.nc ./Ice6g_c_VM5a_10min_0k.nc ./Ice6g_c_VM5a_10min_21k.nc hdpara_step2.nc ~/HDancillarydata ./workdir/ ./outputdir_step2/ aa01 20990
\end{lstlisting}
\end{enumerate}
\section{Guidance for external users} \label{section-external-instructions}

External users will encounter a number of obstacles with running the river routing and parameter generation scripts. Here I try to point out what these might be and give some general guidance as how these might be overcome; however, given the complexity of modern software environments, it is likely that a few hours work will be necessary to successfully run this outside the Max Planck Institute computing environment.

\begin{itemize}
\item The code can be downloaded from either \url{https://github.com/ThomasRiddick/DynamicHD} or \url{https://doi.org/10.5281/zenodo.1208066}. If you are reading this document it is likely you have already done this.
\item It is is necessary to prepare an ancillary data directory before use; I use the name \lstinline[style=bash_input]{HDancillarydata} for this although it can be given another name. The following are needed in the main ancillary data directory:
\begin{itemize}
\item The file \lstinline[style=bash_input]{cotat_plus_standard_params.nl}. The version of this to use is given in \\\lstinline[style=bash_input]{Dynamic_HD_Resources}; simply copy it to the ancillary data directory.
\item The file \lstinline[style=bash_input]{dynamic_hd_production_driver.cfg}. A template is given in \lstinline[style=bash_input]{Dynamic_HD_Resources}; copy it to the ancillary data directory and adapt it appropriately to your needs.
\item  The file \lstinline[style=bash_input]{grid_0_5.txt}. The version of this to use is given in \lstinline[style=bash_input]{Dynamic_HD_Resources}; simply copy it to the ancillary data directory.
\item The file  \lstinline[style=bash_input]{top_level_driver.cfg}. A template is given in \lstinline[style=bash_input]{Dynamic_HD_Resources}; copy it to the ancillary data directory and adapt it appropriately to your needs.
\item The file \lstinline[style=bash_input]{orog_corrs_field_ICE5G_and_tarasov_upscaled_srtm30plus_north_america_only_data_ALG4} \\\lstinline[style=bash_input]{_sinkless_glcc_olson_lsmask_0k_20170517_003802.nc}. Download these relative orography corrections from \url{ https://doi.org/10.5281/zenodo.1326394}, relabel them with this filename and place in the ancillary data directory.
\item Download the present day ICE5G Version 1.2.00 for the present day (0k) orography from \url{http://www.atmosp.physics.utoronto.ca/~peltier/data.php} and place it in the ancillary data directory.
\end{itemize}
\item Within MPI we have a module system which we use to load things such as compilers and the dynamic HD scripts rely on this to ensure the correct compiler and version of anaconda is loaded. There is now an option to switch off all calls to this module system in the scripts. In the \lstinline[style=bash_input]{HDancillarydata} directory add to the the file  \lstinline[style=bash_input]{top level driver.cfg} the line \lstinline[style=bash_input]{no_modules="true"}  (including the quotes; note this file's format is pure bash; it is \lstinline[style=bash_input]{sourced} as opposed to being read). The user will now need to supply the gcc 6.2.0 compiler (\textbf{Warning - some other newer versions of gcc have a known bug with polymorphic variables; using other version of gcc may lead to strange  and very hard to diagnose compilation errors, e.g. the compiler itself crashing}) and `anaconda3' (we are using what is actually Anaconda 4.1.1; conda version 4.3.30 - other versions likely also work but I haven't tested them) unless you are running without conda (see the next subsection).
\item It will be necessary to remove the parameter generation code and hdstart.nc file generation code to leave just the river routing code. First the section \\  \lstinline[style=bash_input]{#Compile fortran code used called shell script wrappers} from \lstinline[style=bash_input]{dynamic_hd_top_level_driver.sh} should be commented out. That is currently lines 330--334 in the script though that could easily change in future releases. Secondly it will be necessary to edit the key python driver \lstinline[style=bash_input]{dynamic_hd_production_driver.py} in \lstinline[style=bash_input]{Dynamic_HD_Scripts/Dynamic_HD_Scripts}; here the sections \lstinline[style=bash_input]{#Generate parameters} and \lstinline[style=bash_input]{#Place parameters and rdirs into a hdparas.file} in the method \lstinline[style=bash_input]{no_intermediaries_ten_minute_data_ALG4_no_true_sinks_plus_upscale_rdirs_driver} should be edited out; these are currently lines 533--548 and 548--570 of that file. but these could easily change in future versions. Some further modification may then be need to clean up the output and convert it to whatever format is required.
\end{itemize}
An alternative possibility is for external users might be to extract the C++ and Fortran code of the main tools and interface directly with its wrappers and rewrite the surrounding glue code in a language and environment of their own choice.
\subsection{Running without Anaconda}

The author strongly recommends the use of the Conda package manager and Anaconda repository as the best tool for managing python packages with a large number of dependencies. However, if unavailable, it should be possible to run the code without this; though this may require a significant amount of work to install all the correct versions of all the correct package manually. Firstly, in the \lstinline[style=bash_input]{HDancillarydata} directory add to the the file  \lstinline[style=bash_input]{top level driver.cfg} the line \lstinline[style=bash_input]{no_conda="true"} to switch off the use of conda in the dynamic hd scripts. Secondly, install and provide the following packages to the script:

\begin{description}
\item[Python 2.7.11] \hfill
\begin{itemize}
\item Download and compile from \\ \url{https://www.python.org/downloads/release/python-2711rc1/}
\end{itemize}
\item[CDO 1.9.4] \hfill
\begin{enumerate}
\item Download from \url{https://code.mpimet.mpg.de/projects/cdo/files}.
\item \lstinline[style=bash_input]{make && make install}
\end{enumerate}
\item[CDO Python Bindings 1.3.6 (must be after CDO)]\hfill
\begin{itemize}
\item Install Python cdo bindings at tag 1.3.6 from \url{https://github.com/Try2Code/cdo-bindings/tree/1.3.6}.
\end{itemize}
\item[cython 0.24] \hfill
\begin{itemize}
\item Use \lstinline[style=bash_input]{pip install --user cython==0.24} (pip is python?s main package manager) or download, unzip/tar and install from \url{https://pypi.org/project/Cython/0.24/#files} using python setup.py install
\end{itemize}
\item[netcdf4 1.4.0] \hfill
\begin{itemize}
\item Download and compile from \url{https://pypi.org/project/netCDF4/1.4.0/#files}
\end{itemize}
\item[scipy 0.17.1 numpy 1.4.0]\hfill
\begin{itemize}
\item Use \lstinline[style=bash_input]{pip install ?user scipy==0.17.1 numpy==1.4.0}
\end{itemize}
\item[xarray 0.10.8]\hfill
\begin{enumerate}
\item Download and install pandas from \url{https://pypi.org/project/pandas/0.23.3/#files}.
\item Install xarray using  \lstinline[style=bash_input]{pip install --user xarray==0.10.8}
\end{enumerate}
\item[matplotlib 1.5.1 (necessary for plotting only - main code should run without it)] \hfill
\begin{enumerate}
\item Install the large number of dependencies of matplotlib (these are just the requirements? may also need to install some of the optional dependencies). A possible alternative might be to use sudo apt-get build-dep python-matplotlib which should install the dependencies automatically. See \url{https://matplotlib.org/users/installing.html#linux} for links:
\begin{enumerate}
\item	Python ($>= 2.7$ or $>= 3.4$) (above)
\item	NumPy ($>= 1.7.1$) (above)
\item	setuptools
\item	dateutil ($>= 2.1$)
\item	pyparsing
\item	libpng ($>= 1.2$)
\item	pytz
\item	FreeType ($>= 2.3$)
\item	cycler ($>= 0.10.0$)
\item	six
\item	backports.functools\_lru\_cache (for Python 2.7 only)
\item	subprocess32 (for Python 2.7 only, on Linux and macOS only)
\item kiwisolver ($>= 1.0.0$)
\end{enumerate}
\item Install matplotlib with \lstinline[style=bash_input]{python -m pip install -U matplotlib==1.5.1}.
\end{enumerate}
\end{description}


\section{Notes} \label{section-notes}
\begin{description}
\item[Diagnostic Output File Control] Diagnostic file output (i.e. things such the catchments and the cumulative flow to cell that are not required to run JSBACH but useful for understanding what is going on with the dynamic HD generation code)  can be controlled the boolean settings in the \path{[output_options]} section of \path{dynamic_hd_production_driver.cfg} in \texttt{HDancillarydata}. \texttt{True} switches the named output on; \texttt{False} switches it off. The  \path{[output_options]} must be present in the \path{dynamic_hd_production_driver.cfg} (whether they are \texttt{True} or \texttt{False}.) Some of the diagnostic output could be reconstructed offline from the hdpara.nc file if it is not produced during the run; however some would require the hdpara.nc generation process to be rerun in order to regenerate.
\item[Input Field Name Control] The names of the field loaded from the input files can also be controlled through settings in \path{dynamic_hd_production_driver.cfg}; this time the settings are place in the \path{[input_fieldname_options]} section. These setting are not compulsory; if not present or set to blank strings then the default field names as described above will be used instead. The four possible field names that can be specified are:
\begin{itemize}
\item \path{input_orography_fieldname}
\item \path{input_landsea_mask_fieldname}
\item \path{input_glacier_mask_fieldname}
\item \path{input_base_present_day_orography_fieldname}
\end{itemize}
\item[Source Code] The top level Bash script calls a Python script which is the primary control routine of the dynamic HD parameter generation code. Within this python script the method  \lstinline[style=bash_input]{setup_and_run_dynamic_hd_para_gen_from_command_line_arguments} calls the method\\  \lstinline[style=bash_input]{no_intermediaries_ten_minute_data_ALG4_no_true_sinks_plus_upscale_rdirs_driver} which is the key routine containing the bulk of the control code for the script.  From this the C++ sink filing code is called via a Cython wrapper and the Fortran 2003 river direction upscaling code is called via f2py Other ancillary tasks are performed by Fortran 90 code called via f2py or via C++ called via a Cython wrapper.. The retention co-efficient generation code is a modified version of Stefan's orginal code for this; this is called from the Python control routine via a bash script as a child process. The C++, Bash shell, Fortran 90, Cython, Fortran 2003 and almost all the Python code are documented in the natural idiom of each language (e.g. document strings for Python).
\item[Environment] The code is design to run on either Mistral or a Linux machine with the MPI-M network. It can also run MacBookPro laptops under macOS Sierra (on which is was largely written) though this would require modification of the top level Bash script. The necessary Python environment is setup automatically using Anaconda by the top level script. The DKRZ/MPI-M module system is also used along with DKRZ/MPI-M's Climate Data Operators (CDOs). The script has been tested with following modules loaded.
\begin{itemize}
\item \lstinline[style=bash_input]{cdo/1.7.0-magicsxx-gcc48}
\item \lstinline[style=bash_input]{imagemagick/6.9.1-7-gcc48}
\item \lstinline[style=bash_input]{pftp/7.4.3.2-0}
\item \lstinline[style=bash_input]{netcdf_c/4.3.2-gcc48}
\end{itemize}
it should work with other combinations of modules but it may be necessary to unload some modules before running (note that unloading is fast; it is just the resetting of environmental variables). Individual experimentation may be need to determine exactly what modules must be removed.
\item[Plots] Plots can be made using the functions contained in \path{Dynamic_HD_Code/Dynamic_HD_Scripts/HD_Plots/plots_library.py}.
\item[External Source Code] This is open source (permissive BSD-style licensed) 3rd party code not installed at MPI-M used for unit testing C++ and FORTRAN.  It is not strictly necessary to actually run the dynamic HD parameter generation but is woven into the build process such that it is not easy to remove. This code is not included in the main git repository. A copy is provided in the \texttt{HDancillarydata} directory some of which must be copied into the main source code to use the C++ and Fortran low and mid-level unit tests. The C++ testing code is Google Test \url{https://github.com/google/googletest} while the FORTRAN testing code is FRUIT \url{https://sourceforge.net/projects/fortranxunit/}.
\item[Unit Tests] Unit tests for most of the Python code and the result of the C++ and Fortran code are given in \path{Dynamic_HD_Code/Dynamic_HD_Scripts/Dynamic_HD_Script_Tests}. The low- and mid-level routines of the C++ code are unit tested with Google Test by running \path{Dynamic_HD_Code/Dynamic_HD_Cpp_Code/Release/Dynamic_HD_Cpp_Exec} while such low- and mid-level routines for the Fortran River Direction Upscaling code are tested with FRUIT by running \path{Dynamic_HD_Code/Dynamic_HD_Fortran_Code/Release/Dynamic_HD_Fortran_Exec}. The FRUIT and Google Test based unit tests need for the necessary files from these two frameworks to be installed in the source code; examine the makefiles to figure out the necessary structure.
\item[Running Multiple Copies of the Script in Parallel] Multiple copies of the script can run using the same ancillary data directory, \lstinline[style=bash_input]{source_directory} and \lstinline[style=bash_input]{external_source_directory} but they must use different working directories. When the preparation and compilation steps are run during the first time-step an exclusive lock is placed on the source code. All other copies of the top level script using the same source code will wait till set-up is complete before running. Any other copies of the top level script running the first time-step will skip the preparation and compilation steps when they resume. While the actual dynamic hydrological discharge code runs (either in the first time-step or any subsequent time-steps) a shared lock is place on the source code. This will block any attempts at to prepare and compile the code and cause them to skip to simply running the code assuming it is already compiled and setup (which it will be if another script is a running a copy of it). In summary multiple copies of the script can share a single set of source code and ancillary data while each running individually as if no other script were running and all issues with compilation and preparation will be handled automatically.
\end{description}
\end{document}
